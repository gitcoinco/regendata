{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 21:38:58,117 - INFO - Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import psycopg2\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    if load_dotenv():\n",
    "        logger.info(\"Loaded .env file\")\n",
    "    else:\n",
    "        logger.info(\"No .env file found or loaded\")\n",
    "except ImportError:\n",
    "    logger.info(\"dotenv not installed, skipping .env file loading\")\n",
    "\n",
    "# Load database credentials from environment variables\n",
    "DB_PARAMS = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'port': os.getenv('DB_PORT'),\n",
    "    'dbname': 'Grants',\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD')\n",
    "}\n",
    "\n",
    "def execute_command(command):\n",
    "    logger.info(f\"Executing command: {command[:50]}...\")  # Log first 50 characters\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = psycopg2.connect(**DB_PARAMS)\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"SET tcp_keepalives_idle = 180;\")  # 3 minutes\n",
    "        cursor.execute(\"SET tcp_keepalives_interval = 60;\")  # 60 seconds\n",
    "        cursor.execute(command)\n",
    "        connection.commit()\n",
    "        logger.info(\"Command executed successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logger.error(f\"Database error: {e}\")\n",
    "        if connection:\n",
    "            connection.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "\n",
    "# Define a function to unnest the 'value' column\n",
    "def clean_model_scores(df):\n",
    "    logger.info(\"Cleaning model scores...\")\n",
    "    # Parse and normalize the JSON data in the specified column\n",
    "    column = 'value'\n",
    "    unnested_df = pd.json_normalize(df[column].apply(json.loads))\n",
    "    logger.info(\"JSON data parsed and normalized.\")\n",
    "    \n",
    "    # Replace '.' with '_' in column names and convert to lowercase\n",
    "    unnested_df.columns = unnested_df.columns.str.replace('.', '_').str.lower()\n",
    "    # Concatenate the unnested dataframe with the original dataframe\n",
    "    df = pd.concat([df.drop(columns=[column]), unnested_df], axis=1)\n",
    "    # Rename columns 'key_0' to 'model' and 'key_1' to 'address'\n",
    "    df.rename(columns={'key_0': 'model', 'key_1': 'address'}, inplace=True)\n",
    "    # Drop specified columns from the dataframe\n",
    "    columns_to_drop = [\n",
    "        'data_meta_version', 'data_meta_training_date', 'data_gas_spent',\n",
    "        'data_n_days_active', 'data_n_transactions', 'data_has_safe_nft'\n",
    "    ]\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    logger.info(\"Model scores cleaned.\")\n",
    "    return df\n",
    "\n",
    "def upload_to_postgres(df, table_name):\n",
    "    temp_table = f\"{table_name}_temp\"\n",
    "    \n",
    "    # Create SQLAlchemy engine\n",
    "    logger.info(f\"Creating SQLAlchemy engine with connection string: postgresql://{DB_PARAMS['user']}:{DB_PARAMS['password']}@{DB_PARAMS['host']}:{DB_PARAMS['port']}/{DB_PARAMS['dbname']}\")\n",
    "    engine = create_engine(f'postgresql://{DB_PARAMS[\"user\"]}:{DB_PARAMS[\"password\"]}@{DB_PARAMS[\"host\"]}:{DB_PARAMS[\"port\"]}/{DB_PARAMS[\"dbname\"]}')\n",
    "    logger.info(\"SQLAlchemy engine created successfully.\")\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Use to_sql to create temporary table and insert data into the temporary table\n",
    "            logger.info(f\"Attempting to write data to temporary table {temp_table}...\")\n",
    "            df.to_sql(temp_table, conn, if_exists='replace', index=False, method='multi', chunksize=1000)\n",
    "            logger.info(f\"Data successfully written to temporary table {temp_table}.\")\n",
    "            \n",
    "            # Log the start of the main table drop operation\n",
    "            logger.info(f\"Attempting to drop main table {table_name}...\")\n",
    "            # Drop the main table if it exists\n",
    "            drop_main_table_query = text(f\"DROP TABLE IF EXISTS {table_name};\")\n",
    "            conn.execute(drop_main_table_query)\n",
    "            logger.info(f\"Main table {table_name} dropped.\")\n",
    "            \n",
    "            # Log the start of the temporary table rename operation\n",
    "            logger.info(f\"Attempting to rename temporary table {temp_table} to {table_name}...\")\n",
    "            # Rename the temporary table to the main table\n",
    "            rename_temp_table_query = text(f\"ALTER TABLE {temp_table} RENAME TO {table_name};\")\n",
    "            conn.execute(rename_temp_table_query)\n",
    "            logger.info(f\"Succesfully renamed temporary table {temp_table} to {table_name}.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write data to database: {e}\")\n",
    "        raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 21:39:27,472 - INFO - Reading parquet file from URL: https://nyc3.digitaloceanspaces.com/regendata/passport/model_scores.parquet\n",
      "2024-12-17 23:03:54,398 - ERROR - Failed to read parquet file from URL: IncompleteRead(337499206 bytes read, 359097 more expected)\n"
     ]
    }
   ],
   "source": [
    "url = 'https://nyc3.digitaloceanspaces.com/regendata/passport/model_scores.parquet'\n",
    "logger.info(f\"Reading parquet file from URL: {url}\")\n",
    "try:\n",
    "    df = pd.read_parquet(url, engine='fastparquet')\n",
    "    logger.info(\"Successfully read parquet file from URL.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to read parquet file from URL: {e}\")\n",
    "    df = None\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
